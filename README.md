# DICOM Image Classifier with Llava-Phi3 (VLM) and SVC

This project presents a complete pipeline to train a Machine Learning model capable of classifying DICOM images. Unlike traditional computer vision approaches, this method uses a **Vision Language Model (VLM)**, specifically **Llava-Phi3** via Ollama, for feature extraction.

The process is based on generating multiple text descriptions for each image and then converting those descriptions into semantic vectors. Finally, a **Support Vector Classifier (SVC)** is trained on these vectors to perform the classification.

## Project Structure

```
.
├── input_dicom/               # <-- Place your original DICOM files here
│   └── ...
├── datos_etiquetados/           # (Created by you after Phase 1)
│   ├── FDT/
│   ├── MTF/
│   └── TOR/
├── dataset_final_vlm/           # (Generated by Phase 2 - VLM)
│   ├── dataset_X.npy
│   └── dataset_y.npy
├── modelo_final_vlm/            # (Generated by Phase 3 - VLM)
│   ├── clasificador_svc.joblib
│   ├── escalador.joblib
│   └── matriz_de_confusion.png
├── fase1_descompresion.py       # Script for Phase 1
├── fase2_generar_vectores_vlm.py # Script for Phase 2 (VLM Version)
├── fase3_entrenamiento_svc.py   # Script for Phase 3
├── fase4_inferencia_vlm.py      # (You will need to create this for inference)
├── config.py                    # Simplified configuration file
├── utils.py                     # Utility functions
└── pyproject.toml               # Project dependencies
```

## Prerequisites

### 1\. Ollama and Llava-Phi3

It is **essential** to have Ollama installed and running.

1.  Install Ollama from [ollama.com](https://ollama.com/).
2.  Download the `llava-phi3` model by running in your terminal:
    ```bash
    ollama run llava-phi3
    ```
    The first time you run it, the model will be downloaded. Once you see the `>>>` prompt, you can exit with `/bye`.

### 2\. Python Dependencies

The required libraries are listed in the `pyproject.toml` file. Make sure to install `ollama` in addition to the others.

```bash
# Example installation with pip
pip install ollama numpy pillow pydicom scikit-learn seaborn joblib gdcm pylibjpeg pylibjpeg-libjpeg
```

## Workflow

### Phase 1 and 1.5: Data Preparation and Labeling

These phases are **identical** to the ResNet version.

1.  **Run `fase1_descompresion.py`**: This will standardize your DICOM files from `input_dicom` into the `f1_descomprimidos` folder, ordering them chronologically by acquisition time.
2.  **Label Manually**: Move the files from `f1_descomprimidos` to their respective class folders (`FDT`, `MTF`, `TOR`) inside `datos_etiquetados`.

### Phase 2: Semantic Vector Extraction with VLM

This is the key phase of this approach. The `fase2_generar_vectores_vlm.py` script uses a **semantic data augmentation** technique:

  * For each image, it generates **50 different text descriptions** by querying the VLM about the image with a variety of prompts.
  * **Differentiated prompts** are used: one set for classes with objects (`MTF`, `TOR`) and another for the background class (`FDT`).
  * Each of these 50 descriptions is converted into a numerical vector (embedding).
  * To prevent memory overload from too many simultaneous requests to Ollama, a **concurrency limit** (`Semaphore`) is used to process files in controlled batches.
  * Finally, all vectors are consolidated into the `dataset_X.npy` and `dataset_y.npy` files.

**➡️ Action:**

1.  Make sure the **Ollama service is running**.
2.  Verify that your labeled images are in the `datos_etiquetados` folder.
3.  Run the script:
    ```bash
    python fase2_generar_vectores_vlm.py
    ```
    **Note:** This process will be considerably slower than the ResNet version, as it involves thousands of calls to the VLM.

**➡️ Result:**

  * A new folder `dataset_final_vlm` will be created with the `dataset_X.npy` and `dataset_y.npy` files.

### Phase 3: Training the SVC Classifier

This phase uses the same training script, but it must be configured to use the dataset generated by the VLM.

**➡️ Action:**

1.  Open the `fase3_entrenamiento_svc.py` script.
2.  **Modify the paths** at the beginning of the script to point to the VLM version directories:
    ```python
    # Directory where the dataset from the previous phase was saved
    DATASET_DIR = BASE_PROJECT_DIR / "dataset_final_vlm"
    # Directory where the final trained model will be saved
    MODEL_OUTPUT_DIR = BASE_PROJECT_DIR / "modelo_final_vlm"
    ```
3.  Run the modified training script:
    ```bash
    python fase3_entrenamiento_svc.py
    ```

**➡️ Result:**

  * The model's performance will be printed to the console.
  * A new folder `modelo_final_vlm` will be created with the classifier (`.joblib`) and scaler (`.joblib`) trained on the VLM data.

### Phase 4: Inference with the VLM Model

To classify a new image, you need a dedicated inference script that replicates the logic of Phase 2 (description + embedding). You can adapt the ResNet inference script (`fase4_inferencia.py`) to do this.

**➡️ Required Logic for `fase4_inferencia_vlm.py`:**

1.  Load the SVC model and scaler from `modelo_final_vlm`.
2.  Read the new DICOM image.
3.  **Generate a description** of the image using Llava-Phi3 (with a generic prompt).
4.  **Get the embedding** of that description.
5.  Scale the resulting vector using the loaded scaler.
6.  Pass it to the SVC model to get the final prediction.

**➡️ Result:**

  * The script will print the final prediction and the confidence for each class, based on the VLM's semantic "understanding".